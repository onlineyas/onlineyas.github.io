# Python and Jupyter Notebooks
## Introduction

Lab 5 is about programming in Humanities. I have seen that scholars in digital humanities mostly code on Jupyter Notebook, using Python language. Here is a hands-on practice to become familiarized with that.

## Questions
**Q1:** What do you notice about how this function has split the string “Okay, okay, ladies, now let’s get in formation, cause I slay”? What has it done that isn’t quite right, and why has it done this? 

Computer thinks that *s* is a word. 
Splitting words using that regular expression, the plit happens on any non-word charachter. 
Space, commas, apasrophes and all non-chars are considered to be non-alpahebtic.

Hower if we use **/s** , only spaces are detected and separation happens based on spaces.
```
import re

def split_into_words(any_chunk_of_text):
    #split the string into individual terms and make them all lowercase
    words = re.split("\s+", any_chunk_of_text.lower())
    return words 

any_chunk_of_text = "I'm doing fine! What about you? let's chat"

all_the_words = split_into_words(any_chunk_of_text)
print(all_the_words)
```

Output:
```
["i'm", 'doing', 'fine!', 'what', 'about', 'you?', "let's", 'chat']
```
**Q2:** What happened? Did it work as you expected? If not, what happened that you didn’t expect? 

```
import re

def split_into_words(any_chunk_of_text):
    #split the string into individual terms and make them all lowercase
    words = re.split("\W+", any_chunk_of_text.lower())
    return words 

any_chunk_of_text = "I'm doing fine! What about you? let's chat"

all_the_words = split_into_words(any_chunk_of_text)
print(all_the_words)
```

This is the **list** printed:
```
['i', 'm', 'doing', 'fine', 'what', 'about', 'you', 'let', 's', 'chat']
```
Nothing unexpected happened. Computer did as expected like Q1.


**Q3:** Describe the output of this script (the dataframe that displays after the above cell finishes running). Remember that this is the same output as the “vir-ver-counts-specific” spreadsheet in our Lab5 Google drive folder, only for just 10 texts.

The output is a table(or some lines) containing .xml file names that we collected in a list. The code searched for tags such as <author>, <title> and <dates>, formatted them, and matched the information together so we could see each in one row. The point of the code, besides cleaning the data, so we can match and do the counting possible, such as done in the "date format" part of the code, for example, was to actually count the *virtu and *vertu words. So those two final columns, including the count of the targeted words, are why we did all this! 

**Q4:** Look at the below lines from the compare_counts_specific function above. These lines use regular expressions to do something to the value of the <date> field in an xml file (if the contents of the <date> field meet certain conditions, that is). What are these lines doing? 
    
    `
    if re.search(r'^20', date):
    
    date = re.sub('20', '', date)

    `
  
 To the secondary question of how to know what a caret does, I can say that we can refer to re library [here](https://docs.python.org/3/library/re.html).   
 > ^
> (Caret.) Matches the start of the string, and in MULTILINE mode also matches immediately after each newline.   

    
    We have already grabbed the value of date by the line below in the previous code:
    `
    date = date.get_text()
    `
    
    Searching among dates, any date beginning with 20 (example: 20XY) is found and is substituted with nothing, meaning it has been removed. If the command is doing so, I assume this: removing all dates with this format: 20XY. 

    I was not very familiar with EEBO up to this point. After going through the website, I noticed that this information is all relevant to early English books, meaning definitely before 2000! (1475 through to 1700)[^1].
    I speculate that these dates might be the uploading dates.[^2] So the code is doing all of us a favor and removing all uploading dates because, as the code explanation says, in this context, we might lose some actual publishing dates (meaning not having them at the beginning at all), and this must be all right in this case due to the research question. Plus,  uploading dates seemingly does not matter much.
 All in all, cleaning dates was proved to be very tough here in this case.

    
## Reflections & Discussions
- Discuss your experience exploring Python in this week’s lab in relation to at least one of our readings assigned for this week (week 6). This discussion should be specific but it needn’t be long (i.e., 2-4 paragraphs).
    
    
    
## Dataset for Lab 6
[Trump Twitter Archive](https://www.thetrumparchive.com/) 
    
    
[^1]:EEBO: [About the texts](https://textcreationpartnership.org/tcp-texts/eebo-tcp-early-english-books-online/#)
[^2]: *From 2000-2009, Phase I successfully converted 25,000+ selected texts from the Early English Books Online corpus. * from EEBO: [About the texts](https://textcreationpartnership.org/tcp-texts/eebo-tcp-early-english-books-online/#)
