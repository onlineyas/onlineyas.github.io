# Python and Jupyter Notebooks
## Introduction

Lab 5 is about programming in Humanities. I have seen that scholars in digital humanities mostly code on Jupyter Notebook, using Python language. Here is a hands-on practice to become familiarized with that.

## Questions
**Q1:** What do you notice about how this function has split the string “Okay, okay, ladies, now let’s get in formation, cause I slay”? What has it done that isn’t quite right, and why has it done this? 

Computer thinks that *s* is a word. 
Splitting words using that regular expression, the plit happens on any non-word charachter. 
Space, commas, apasrophes and all non-chars are considered to be non-alpahebtic.

Hower if we use **/s** , only spaces are detected and separation happens based on spaces.
```
import re

def split_into_words(any_chunk_of_text):
    #split the string into individual terms and make them all lowercase
    words = re.split("\s+", any_chunk_of_text.lower())
    return words 

any_chunk_of_text = "I'm doing fine! What about you? let's chat"

all_the_words = split_into_words(any_chunk_of_text)
print(all_the_words)
```

Output:
```
["i'm", 'doing', 'fine!', 'what', 'about', 'you?', "let's", 'chat']
```
**Q2:** What happened? Did it work as you expected? If not, what happened that you didn’t expect? 

```
import re

def split_into_words(any_chunk_of_text):
    #split the string into individual terms and make them all lowercase
    words = re.split("\W+", any_chunk_of_text.lower())
    return words 

any_chunk_of_text = "I'm doing fine! What about you? let's chat"

all_the_words = split_into_words(any_chunk_of_text)
print(all_the_words)
```

This is the **list** printed:
```
['i', 'm', 'doing', 'fine', 'what', 'about', 'you', 'let', 's', 'chat']
```
Nothing unexpected happened. Computer did as expected like Q1.


**Q3:** Describe the output of this script (the dataframe that displays after the above cell finishes running). Remember that this is the same output as the “vir-ver-counts-specific” spreadsheet in our Lab5 Google drive folder, only for just 10 texts.

The output is a table(or some lines) containing .xml file names that we collected in a list. The code searched for tags such as <author>, <title> and <dates>, formatted them, and matched the information together so we could see each in one row. The point of the code, besides cleaning the data, so we can match and do the counting possible, such as done in the "date format" part of the code, for example, was to actually count the *virtu and *vertu words. So those two final columns, including the count of the targeted words, are why we did all this! 

**Q4:** Look at the below lines from the compare_counts_specific function above. These lines use regular expressions to do something to the value of the <date> field in an xml file (if the contents of the <date> field meet certain conditions, that is). What are these lines doing? 
    
    
    if re.search(r'^20', date):
    
    date = re.sub('20', '', date)

    
  
 To the secondary question of how to know what a caret does, I can say that we can refer to *re* library [here](https://docs.python.org/3/library/re.html):   
    
     
    
> (Caret.)^ *Matches the start of the string, and in MULTILINE mode also matches immediately after each newline*.  
    
 
We have already grabbed the value of date by the line below in the previous code:
    
```
    date = date.get_text()
```    
    
 Searching among dates, any date beginning with 20 (example: 20XY) is found and is substituted with nothing, meaning it has been removed. If the command is doing so, I assume this: removing all dates with this format: 20XY. 

I was not very familiar with EEBO up to this point. After going through the website, I noticed that this information is all relevant to early English books, meaning definitely before 2000! (1475 through to 1700)[^1]. I speculate that these dates might be the uploading dates.[^2] So the code is doing all of us a favor and removing all uploading dates because, as the code explanation says, in this context, we might lose some actual publishing dates (meaning not having them at the beginning at all), and this must be all right in this case due to the research question. Plus,  uploading dates seemingly does not matter much. All in all, cleaning dates was proved to be very tough here in this case.

    
## Reflections & Discussions
- Discuss your experience exploring Python in this week’s lab in relation to at least one of our readings assigned for this week (week 6). This discussion should be specific but it needn’t be long (i.e., 2-4 paragraphs).
    
   
   I believe Schmidt's piece and the coinage of *transformations* is the theme word of this session. I ALWAYS wondered what humanists who can code can do in the field, and do they really need to know how to code? More importantly, can people with coding skills from CS do what humanists aim to do in the DH field? The answer seems to be more clear now. I assume not! Humanists are needed to be there because they know the text, its history, and all about it. The data we're dealing with is sensitive. You can try to explain to a coder what you want, but you need to know the appropriate language; first and second, the coder needs to be familiar with the materials, quite as much as you do! This hasn't been easy and always convenient? That's where DH comes in. That's why humanists need to understand the codes, even if they can't write them very efficiently. Even if they can't create clean, fast codes, they need to know the language* and the very essence of an *algorithm* to be able to play with their materials!
    
    When I was reading the long code of this lab, I was not focused on how to create it later (which was the main point in engineering and coding classes). Instead, for the very first time, I tried to track the variables and what was happening to them. Comments were helpful and trustworthy. I knew that Dr. Thomas had written them, and she's the writer of the codes herself, too. (it's important that you can understand the comments and trust them!) I liked this lab the most because, together with the readings, it gave me a realistic idea of how coding is in the world of humanities: it's project-based. Your project demands it, and you go learn it! You need to know this language, having reading comprehension, not necessarily writing comprehension skills!
     
## Dataset for Lab 6
[Trump Twitter Archive](https://www.thetrumparchive.com/) 
    
    
[^1]:EEBO: [About the texts](https://textcreationpartnership.org/tcp-texts/eebo-tcp-early-english-books-online/#)
[^2]: "*From 2000-2009, Phase I successfully converted 25,000+ selected texts from the Early English Books Online corpus.*" from EEBO: [About the texts](https://textcreationpartnership.org/tcp-texts/eebo-tcp-early-english-books-online/#)
